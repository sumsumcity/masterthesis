[
    {
        "Threat Category": "Nefarious Activity/Abuse",
        "Threat": "Access Control List (ACL) manipulation",
        "Description": "In AI data collection scenarios, group-based ACLs for datasets may fail when the nesting of large groups is changed. Given a data set and a group of sources Sensor_Group_A which has been granted access to update it, it is easy to check if an individual user or sensor is a member of Sensor_Group_A and inherits the corresponding permissions. However, if Sensor_Group_A is joined as a member to many other groups, inherited permissions become difficult to check for each sensor and escalation of privileges of untrusted sources may result. The threat involves Implicit privilege elevation attacks take advantage of group nesting modifications to upscale access permissions for specific users",
        "Potential Impact": "Integrity",
        "Affected assets": "Artefacts"
    },
    {
        "Threat Category": "Nefarious Activity/Abuse",
        "Threat": "Adversarial examples",
        "Description": "Targeting the inference phase of ML and deep learning systems that AI is based on is one of the most prominent and highly publicized threats. Adversarial examples refer to data that include perturbations that are imperceptible to the human eye, but that can have an impact on the effectiveness and performance of ML models.",
        "Potential Impact": "Integrity, Availability",
        "Affected assets": "Model, Data"
    },
    {
        "Threat Category": "Nefarious Activity/Abuse",
        "Threat": "Backdoor/insert attacks on training datasets",
        "Description": "Threaten the ML model’s integrity by trying to introduce spurious inferences. Attackers introduce special trigger patterns in part of the training data, and presenting the trigger in the inference phase will cause targeted misclassifications. For example, an attacker can introduce in the training data of an image classifier connected to a surveillance camera an example including a certain pixel pattern and the label “policeman”. Once the classifier is trained and deployed, the attacker wears a t-shirt with that pattern and passes by the camera with a gun in hand without triggering any alarm.",
        "Potential Impact": "Integrity",
        "Affected assets": "Model, Data"
    },
    {
        "Threat Category": "Nefarious Activity/Abuse",
        "Threat": "Compromising AI inference's correctness - data",
        "Description": "This type of threats refers to possible exploitations involving either data manipulation, or selection bias in raw data, or modification of labels and deletion or omission of labelled data items. It may also refer to compromising AI correctness by insertion of adversarial data (poisoned/manipulated) in augmented data sets, as well as by means of interruption of training or modification of model parameters.",
        "Potential Impact": "Integrity",
        "Affected assets": "Data"
    },
    {
        "Threat Category": "Nefarious Activity/Abuse",
        "Threat": "Compromise and limit AI results",
        "Description": "This type of threat can emerge due to involuntary or unintentionally actions from Data Owners, that may hide data due to business secrets or by not recognizing its value; by AI/ML designers and engineers, that can intentionally tamper or, due to lack of experience, miss to include data. This threat may also be related to AI/ML service users not being able to understand the model capabilities and/or results.",
        "Potential Impact": "Integrity, Availability",
        "Affected assets": "Model, Actor, Artefacts"
    },
     {
        "Threat Category": "Nefarious Activity/Abuse",
        "Threat": "Compromising ML inference's correctness – algorithms",
        "Description": "Threats to the availability of the ML training algorithm, as well as threats that aim at compromising the training algorithm to adversely affect the desired accuracy.",
        "Potential Impact": "Integrity, Availability",
        "Affected assets": "Model"
    },
    {
        "Threat Category": "Nefarious Activity/Abuse",
        "Threat": "Compromising ML pre-processing",
        "Description": "Flaws or defects of the data and metadata schemata greatly influence the quality of the analysis by applications that use the data. In AI applications, a flawed schema will negatively impact on the quality of the ingested information. Flaws often result from inconsistencies in the use of modeling methodologies, but may also depend on intentional schema poisoning, i.e. any manipulation of a schema intended to compromise the programs that ingest or pre-process data that use it. It is also possible for adversaries to mount Schema-based denial of service attacks, which modify the data schema so that it does not contain required information for subsequent processing.",
        "Potential Impact": "Integrity, Availability",
        "Affected assets": "Data"
    },
    {
        "Threat Category": "Nefarious Activity/Abuse",
        "Threat": "Compromising ML training – augmented data",
        "Description": "Threats to augmented datasets due to inconsistency with the training set they are derived from, and specifically when highly diverse, automatically generated data are added to a data set of collected data, which are very consistent but highly representative of their application domain, so there would be no need to limit overfitting. Enriching data always entails some risks. This threat can lead to non-satisfaction of functional requirements, i.e. poor inference.",
        "Potential Impact": "Integrity, Availability, Confidentiality",
        "Affected assets": "Data"
    },
    {
        "Threat Category": "Nefarious Activity/Abuse",
        "Threat": "Compromising ML training – validation data",
        "Description": "This threat refers to shortening the training of the ML model dramatically by compromising the integrity of the validation dataset. It also includes the generation of adversarial validation data that are quite different from genuine training set data.",
        "Potential Impact": "Integrity, Availability",
        "Affected assets": "Data"
    },
    {
        "Threat Category": "Nefarious Activity/Abuse",
        "Threat": "Compromise of data brokers/providers",
        "Description": "This threat refers to compromising data brokers/providers to influence the machine learning process as they can deliberately or accidentally manipulate the data sent to the AI process, in several different ways: poisoning-via-insertion of malicious data; deleting registries to eliminate features either by changing the data, removing part of it or adding new registries. In addition, sometimes the mere data availability prevails over any consideration on data quality, with the risk that learning models are fed with data streams that do not reflect the statistical characteristics of a phenomenon and determining likely biases in the subsequent decisional processes.",
        "Potential Impact": "Integrity, Availability",
        "Affected assets": "Actor"
    },
    {
        "Threat Category": "Nefarious Activity/Abuse",
        "Threat": "Compromise of model frameworks",
        "Description": "Model frameworks fail when are misconfigured or offer additional attack vectors with respect to traditional software, firmware, and hardware environments. The ML platform’s data volume and processing requirements mean that the workloads are often handled on the cloud, adding another level of complexity and vulnerability. Moreover, the threat of backdoors in libraries is also evident, similarly to the potential threats of attacks on model input and output data can be performed on the hardware, firmware, Operating System (OS) and software level.",
        "Potential Impact": "Integrity, Availability",
        "Affected assets": "Artefacts, Environment/tools"
    },
    {
        "Threat Category": "Nefarious Activity/Abuse",
        "Threat": "Corruption of data indexes",
        "Description": "Data indexes threats manifest when their content becomes corrupted. Corruption may be the result of an attack, or due to system crashes or loss of network connectivity during index replication. The same events may cause interruptions of index construction tasks, bringing a partially built (and therefore defective) index to production. Also, running out of storage capacity during indexing or replication may cause an entire data index to be deleted. Denial-of-service attacks to indexes intentionally corrupt data indexes to decrease the performance of data access. Additionally, timing attacks to indexes use access time to public items before and after inserting them (which depend on the index content) to infer the presence and size of inaccessible data items.",
        "Potential Impact": "Integrity, Availability, Confidentiality",
        "Affected assets": "Artefacts"
    },
    {
        "Threat Category": "Nefarious Activity/Abuse",
        "Threat": "Data poisoning",
        "Description": "This threat relates to the injection of erroneous/tampered/wrong data in the training set or the validation set by either getting legitimate access or illegitimate one through exploiting poor authentication/authorization mechanisms. The aim is to adversely affect the operation of the AI system.",
        "Potential Impact": "Integrity, Availability",
        "Affected assets": "Processes, Environment/tools, Model"
    },
    {
        "Threat Category": "Nefarious Activity/Abuse",
        "Threat": "Data tampering",
        "Description": "Actors like AI/ML designers and engineers can deliberately or unintentionally manipulate and expose data. Data can also be manipulated during the storage procedure and by means of some processes like feature selection. Besides interfering with model inference, this type of threat can also bring severe discriminatory issues by introducing bias.",
        "Potential Impact": "Availability, Integrity",
        "Affected assets": "Processes, Environment/tools, Model"
    },
    {
        "Threat Category": "Nefarious Activity/Abuse",
        "Threat": "DDoS",
        "Description": "Distributed Denial of Service attacks may be utilized by adversaries to reduce the availability of online IT systems and distributed file systems (e.g. cloud storage) used to support AI systems and their operation.",
        "Potential Impact": "Availability",
        "Affected assets": "Environment/tools"
    },
    {
        "Threat Category": "Nefarious Activity/Abuse",
        "Threat": "Elevation-of-Privilege",
        "Description": "These threats refer to exploiting trained and tuned models to gain access to parameter values and even to understand whether some data was part of the data set used.",
        "Potential Impact": "Confidentiality, Availability",
        "Affected assets": "Model, Data"
    },
    {
        "Threat Category": "Nefarious Activity/Abuse",
        "Threat": "Insider threat",
        "Description": "AI designers and developers may deliberately expose data and models for a variety of reasons, e.g. revenge or extortion. Integrity, data confidentiality and trustworthiness are the main impacted security properties.",
        "Potential Impact": "Integrity, Confidentiality, Availability",
        "Affected assets": "Actor"
    },   {
        "Threat Category": "Nefarious Activity/Abuse",
        "Threat": "Introduction of selection bias",
        "Description": "Data owners may introduce selection bias on purpose when publishing raw data in order to adversely affect inference to be drawn on the data.",
        "Potential Impact": "Integrity, Availability",
        "Affected assets": "Data"
    },
    {
        "Threat Category": "Nefarious Activity/Abuse",
        "Threat": "Label manipulation or weak labeling",
        "Description": "This threat refers to supervised learning systems, which may not infer correctly due to wrong or imprecise data labels. Adversaries can modify the training labels, perturb labels, or introduce adversarial label noise to disrupt model inference.",
        "Potential Impact": "Availability, Integrity",
        "Affected assets": "Processes, Data"
    },
    {
        "Threat Category": "Nefarious Activity/Abuse",
        "Threat": "Manipulation of data sets and data transfer process",
        "Description": "These threats are seen in the context of storage of data sets in infrastructures provided by third parties, which make them remotely accessible. The threat refers to manipulation and tampering of the data stored and manipulation of the data transfer process.",
        "Potential Impact": "Confidentiality, Integrity",
        "Affected assets": "Environment/tools"
    },
    {
        "Threat Category": "Nefarious Activity/Abuse",
        "Threat": "Manipulation of labeled data",
        "Description": "Threats to labeled data items occur when enough labels and data are deleted/omitted, when a sufficient number of spurious labeled data is included in the data set, or when enough labels are modified. All such modifications affect the model training and inference.",
        "Potential Impact": "Integrity",
        "Affected assets": "Data"
    },
    {
        "Threat Category": "Nefarious Activity/Abuse",
        "Threat": "Manipulation of model tuning",
        "Description": "Adversaries may fine-tune hyper-parameters and thus influence the AI system’s behavior. Changes to hyper-parameters can lead to overfitting or insider attacks. The usage of default hyper-parameters may increase the transferability of adversarial attacks.",
        "Potential Impact": "Integrity, Availability",
        "Affected assets": "Processes, Model"
    },
    {
        "Threat Category": "Nefarious Activity/Abuse",
        "Threat": "Manipulation of optimization algorithm",
        "Description": "Optimization algorithms are often used in the context of processes like Model Tuning to set up hyper-parameters values. Nefarious abuse of such algorithms by adversaries may lead to erroneous tuning of models.",
        "Potential Impact": "Availability",
        "Affected assets": "Model, Processes"
    },
    {
        "Threat Category": "Nefarious Activity/Abuse",
        "Threat": "Misclassification based on adversarial examples",
        "Description": "This threat involves the manipulation of model parameters or the use of adversarial examples during inference to force misclassification of model results. Actors with access to models and datasets, such as Cloud providers, model providers, and model users, are involved in this threat. Misclassification can also be instigated by Processes assets, such is the case of using adversarial examples during training and transfer learning stages, as well as during the inference stage.",
        "Potential Impact": "Integrity, Availability",
        "Affected assets": "Actor, Processes"
    },
    {
        "Threat Category": "Nefarious Activity/Abuse",
        "Threat": "ML model confidentiality",
        "Description": "This threat refers to exploitation of the ML model to leak (in its outputs or otherwise) some information about its internal parameters or performance to unauthorized parties.",
        "Potential Impact": "Confidentiality",
        "Affected assets": "Model"
    },
    {
        "Threat Category": "Nefarious Activity/Abuse",
        "Threat": "ML model integrity manipulation",
        "Description": "This threat refers to manipulation of the ML model by delivering output values that were not generated based on its internal parameters or by delivering overtly biased or useless outputs (e.g. constant, or indistinguishable from random noise).",
        "Potential Impact": "Integrity",
        "Affected assets": "Model"
    },
    {
        "Threat Category": "Nefarious Activity/Abuse",
        "Threat": "Model backdoors",
        "Description": "It is often the case that 3rd parties provide models (called as “Teacher” models), previously trained and fine-tuned with large datasets that are useful to learn from small datasets and/or by organizations without access to high computational clusters. The resulting models may be subject to backdoor threats that expose their inner working (breach of confidentiality), impact their operation (integrity breach) or degrade/cancel their performance (impact on availability).",
        "Potential Impact": "Integrity, Availability, Confidentiality",
        "Affected assets": "Processes"
    },
    {
        "Threat Category": "Nefarious Activity/Abuse",
        "Threat": "Model poisoning",
        "Description": "This threat refers to a legitimate model file being replaced entirely by a poisoned model file. In the context of AI as a Service, with many types of data and code being uploaded on cloud infrastructures, this threat may be realized by exploiting potential weaknesses of cloud providers.",
        "Potential Impact": "Integrity, Availability",
        "Affected assets": "Model, Actor"
    },
    {
        "Threat Category": "Nefarious Activity/Abuse",
        "Threat": "Model Sabotage",
        "Description": "Sabotaging the model is a nefarious threat that refers to the exploitation or physical damage of libraries and machine learning platforms that host or supply AI/ML services and systems.",
        "Potential Impact": "Availability, Integrity",
        "Affected assets": "Environment/tools, Model"
    },
    {
        "Threat Category": "Nefarious Activity/Abuse",
        "Threat": "Online system manipulation",
        "Description": "This is related to model replacement by a malicious backdoored model, used for targeted or non-targeted attack, which can be exploited by Actors like Cloud Providers during Processes like model training or transfer.",
        "Potential Impact": "Confidentiality, Integrity",
        "Affected assets": "Model"
    },
    {
        "Threat Category": "Nefarious Activity/Abuse",
        "Threat": "Overloading/confusing labelled dataset",
        "Description": "Append attacks target availability by adding random samples to the training set to the point of preventing any model trained on that dataset from computing any meaningful inference. The threat may lead to overfitting or underfitting the labeled dataset.",
        "Potential Impact": "Availability",
        "Affected assets": "Data"
    },
    {
        "Threat Category": "Nefarious Activity/Abuse",
        "Threat": "Reducing data accuracy",
        "Description": "This threat refers to the reduction of the degree of data accuracy, by directly modifying the data or by mixing datasets with highly different degrees of quality.",
        "Potential Impact": "Integrity, Availability",
        "Affected assets": "Data"
    },
    {
        "Threat Category": "Nefarious Activity/Abuse",
        "Threat": "Reduce effectiveness of AI/ML results",
        "Description": "Users can make erroneous usage of AI services, either for not having a good understanding about the model capabilities or by not being able to understand when changes in the process imply model maintenance, and possibly re-training procedures. End-users can modify the input data to the model that results in “de-training” the model.",
        "Potential Impact": "Integrity, Availability",
        "Affected assets": "Processes, Actor"
    },
     {
        "Threat Category": "Nefarious Activity/Abuse",
        "Threat": "Sabotage",
        "Description": "Sabotage involves intentionally destroying or maliciously affecting the IT infrastructure that supports AI systems.",
        "Potential Impact": "Availability",
        "Affected assets": "Environment/tools"
    },
    {
        "Threat Category": "Nefarious Activity/Abuse",
        "Threat": "Scarce data",
        "Description": "AI relies on the availability of consistent and accessible data. This threat involves data scarcity (deliberately created by an adversary) that may compromise AI viability and/or compromise and limit its results. This can be exploited deliberately (for nefarious activities) or unintentionally during Data Ingestion.",
        "Potential Impact": "Availability",
        "Affected assets": "Data, Processes"
    },
    {
        "Threat Category": "Nefarious Activity/Abuse",
        "Threat": "Transferability of adversarial attacks",
        "Description": "ML and deep learning models are mostly based on an inductive approach to problem solving, as opposed to the deductive approach of traditional mathematical modeling. This means that experience matters and not always it can be given for granted that ML models can be smoothly transferred and applied in a new scenario and for a new AI application. This threat refers to adversarial examples that may be transferred to AI/ML applications and Environment/Tools like AI/ML libraries and machine learning platforms.",
        "Potential Impact": "Integrity",
        "Affected assets": "Process, Environment/tools"
    },
    {
        "Threat Category": "Nefarious Activity/Abuse",
        "Threat": "Unauthorized access to data sets and data transfer process",
        "Description": "These threats are seen in context of storage of data sets in infrastructures provided by third parties, which make them remotely accessible. The threat refers to unauthorized access of the data stored and unauthorized access to the inner workings of the data transfer process.",
        "Potential Impact": "Confidentiality, Integrity",
        "Affected assets": "Environment/tools"
    },
    {
        "Threat Category": "Nefarious Activity/Abuse",
        "Threat": "Unauthorized access to models’ code",
        "Description": "This threat refers to machine learning libraries and machine learning platforms being manipulated to inject malicious code that will exploit users' models and gain access to datasets.",
        "Potential Impact": "Confidentiality, Integrity",
        "Affected assets": "Environment/tools"
    },
    {
        "Threat Category": "Nefarious Activity/Abuse",
        "Threat": "White-box, targeted or non-targeted",
        "Description": "This threat refers to misclassification to a specific target class or to a different class rather than the correct one. This type of threat is mainly associated with Processes assets like Model Selection/Building, Training, Testing, Transfer Learning and Model Deployment and can be exploited by Actors such as Model Providers.",
        "Potential Impact": "Integrity, Availability",
        "Affected assets": "Processes"
    },
    {
        "Threat Category": "Unintentional Damage",
        "Threat": "Bias introduced by data owners",
        "Description": "Data Owners may try to hide information that will be fed to the AI systems as part of their business interests. Moreover, they are also people that may be biased themselves, as they tend to be far from raw data and may be incapable of giving good data to the models. This type of threat can severely affect trustworthiness of AI systems.",
        "Potential Impact": "Availability, Integrity",
        "Affected assets": "Actor"
    },
    {
        "Threat Category": "Unintentional Damage",
        "Threat": "Compromising AI inference's correctness - data",
        "Description": "This type of threat refers to possible exploitations involving either data manipulation, or unintentional selection bias in raw data, or modification of labels and deletion or omission of labelled data items. It may also refer to compromising AI correctness by insertion of adversarial data in augmented data sets, as well as by means of interruption of training or modification of model parameters.",
        "Potential Impact": "Integrity",
        "Affected assets": "Data"
    },
    {
        "Threat Category": "Unintentional Damage",
        "Threat": "Compromise and limit AI results",
        "Description": "This type of threat can emerge due to involuntary or unintentionally actions from Data Owners, that may hide data due to business secrets or by not recognizing its value; by AI/ML designers and engineers, that can intentionally tamper or, due to lack of experience, miss to include data. This threat may also be related to AI/ML service users not being able to understand the model capabilities and/or results.",
        "Potential Impact": "Integrity, Availability",
        "Affected assets": "Model, Actor, Artefacts"
    },
    {
        "Threat Category": "Unintentional Damage",
        "Threat": "Compromising ML inference's correctness – algorithms",
        "Description": "Threats to the availability of the ML training algorithm, as well as threats that aim at compromising the training algorithm to adversely affect the desired accuracy.",
        "Potential Impact": "Integrity, Availability",
        "Affected assets": "Model"
    },
    {
        "Threat Category": "Unintentional Damage",
        "Threat": "Compromising ML training – augmented data",
        "Description": "Threats to augmented datasets due to inconsistency with the training set they are derived from, and specifically when highly diverse, automatically generated data are added to a data set of collected data, which are very consistent but highly representative of their application domain, so there would be no need to limit overfitting. Enriching data always entails some risks. This threat can lead to non-satisfaction of functional requirements, i.e. poor inference.",
        "Potential Impact": "Availability, Integrity",
        "Affected assets": "Data"
    },
    {
        "Threat Category": "Unintentional Damage",
        "Threat": "Compromising feature selection",
        "Description": "This threat refers to performance degradation of feature selection algorithms by delivering feature sets that are strongly predictive only for some classes, neglecting other features that are needed to discriminate difficult classes.",
        "Potential Impact": "Integrity, Availability",
        "Affected assets": "Model"
    },
    {
        "Threat Category": "Unintentional Damage",
        "Threat": "Compromise of data brokers/providers",
        "Description": "This threat refers to compromising data brokers/providers to influence the machine learning process as they can deliberately or accidentally manipulate the data sent to the AI process, in several different ways: poisoning-via-insertion of malicious data; deleting registries to eliminate features either by changing the data, removing part of it or adding new registries. In addition, sometimes the mere data availability prevails over any consideration on data quality, with the risk that learning models are fed with data streams that do not reflect the statistical characteristics of a phenomenon and determining likely biases in the subsequent decisional processes.",
        "Potential Impact": "Integrity, Availability",
        "Affected assets": "Actor"
    },
    {
        "Threat Category": "Unintentional Damage",
        "Threat": "Compromise of model frameworks",
        "Description": "Model frameworks fail when they are misconfigured or offer additional attack vectors with respect to traditional software, firmware, and hardware environments. The ML platform’s data volume and processing requirements mean that the workloads are often handled on the cloud, adding another level of complexity and vulnerability.",
        "Potential Impact": "Integrity, Availability",
        "Affected assets": "Artefacts, Environment/tools"
    },
    {
        "Threat Category": "Unintentional Damage",
        "Threat": "Compromise privacy during data operations",
        "Description": "Data modification or erroneous handling during processes like Data Exploration or Pre-Processing may lead to unintentional data breaches respectively and accordingly lead to legal concerns over privacy breaches.",
        "Potential Impact": "Confidentiality",
        "Affected assets": "Data"
    },
    {
        "Threat Category": "Unintentional Damage",
        "Threat": "Disclosure of personal information",
        "Description": "At all stages of the AI lifecycle, disclosure of personal information (either directly or by means of correlation) is a noteworthy threat. The threat is particularly manifested in the absence of verified data accuracy of sources, lack of data randomization, lack of pseudonymity mechanisms, etc.",
        "Potential Impact": "Confidentiality",
        "Affected assets": "Data, Model, Processes, Actor, Artefacts, Environment/tools"
    },
    {
        "Threat Category": "Unintentional Damage",
        "Threat": "Erroneous configuration of models",
        "Description": "This type of threat materializes when models are used recklessly by end users (but also AI experts) without proper consideration of contextual factors that may not fit with the phenomenon being analyzed. Lack of expertise and proper knowledge of AI models’ operation are the main cause of these erroneous configurations.",
        "Potential Impact": "Integrity, Availability",
        "Affected assets": "Processes, Actor"
    },
    {
        "Threat Category": "Unintentional Damage",
        "Threat": "Label manipulation or weak labeling",
        "Description": "This threat refers to supervised learning systems, which may not infer correctly due to wrong or imprecise data labels. Messing with the labels may introduce the same effects of threats that are pertinent to adversaries attacking the labeling process.",
        "Potential Impact": "Availability",
        "Affected assets": "Processes, Data"
    },
    {
        "Threat Category": "Unintentional Damage",
        "Threat": "Lack of sufficient representation in data",
        "Description": "Raw data assets fail when they are not sufficiently representative of the domain or unfit for the AI business goal, e.g. due to sample size and population characteristics. Data size does not always imply representativeness. If data selection is biased towards some elements that have similar characteristics (selection bias) then even a large sample will not deliver representative data. Assessment of data representativeness cannot be done a priori; it is only possible after identifying the targeted population and the purpose for collecting the data. Selection bias can be alleviated by employing balanced sampling techniques. Correction of existing data sets does, however, require information regarding the existence and nature of the bias; when selection bias is unknown to the AI model designer, no correction-based approaches to the inference process are possible.",
        "Potential Impact": "Availability",
        "Affected assets": "Data"
    },
     {
        "Threat Category": "Unintentional Damage",
        "Threat": "Manipulation of labelled data",
        "Description": "Unintentional threats to labelled data items occur when enough numbers of labels and data are deleted/omitted by mistake, when a sufficient number of spurious labelled data is included into the data set, or when enough labels are modified. Since the labelled data set is used for the purpose of training a ML model, all such modifications affect the model training and inference (e.g., shifting the model’s classification boundary).",
        "Potential Impact": "Integrity, Confidentiality",
        "Affected assets": "Data"
    },
    {
        "Threat Category": "Unintentional Damage",
        "Threat": "Misconfiguration or mishandling of AI system",
        "Description": "AI designers and developers may unintentionally expose data and models or may even misconfigure an AI system by mistake. Data confidentiality and trustworthiness are the main impacted security properties.",
        "Potential Impact": "Confidentiality",
        "Affected assets": "Actor"
    },
    {
        "Threat Category": "Unintentional Damage",
        "Threat": "Mishandling of statistical data",
        "Description": "This may happen, for instance, if maximum likelihood predictions are drawn from the sample, correctly reflecting the way the majority of individuals express a specific parameter that may not mirror the way the minority will be affected by the prediction. Also, other forms of unintended bias may take place. For instance, in ranking algorithms even if parameters under analysis may be ranked fairly and in the correct order, the rewards allocated to each “slot” (click through rates, impressions or any other sort of share of “good” to allocate) may not be fairly distributed, with limited possibility to rebalance such uneven situations.",
        "Potential Impact": "Availability, Confidentiality",
        "Affected assets": "Data"
    },
    {
        "Threat Category": "Unintentional Damage",
        "Threat": "ML Model Performance Degradation",
        "Description": "The performance of an AI’s system ML model may degrade due to the data governance policy, by omission or by corruption due to system crashes or loss of network connectivity.",
        "Potential Impact": "Availability, Confidentiality",
        "Affected assets": "Processes, Model"
    },
    {
        "Threat Category": "Unintentional Damage",
        "Threat": "Online system manipulation",
        "Description": "This is related to model replacement by a backdoored model by mistake, used for targeted or non-targeted attack. This can be the result of unintended actions by Actors like Cloud Providers during Processes like model training or transfer.",
        "Potential Impact": "Confidentiality, Integrity",
        "Affected assets": "Model"
    },
    {
        "Threat Category": "Unintentional Damage",
        "Threat": "Reducing data accuracy",
        "Description": "This threat refers to the reduction of the degree of data accuracy, by directly modifying the data or by mixing datasets with highly different degrees of quality.",
        "Potential Impact": "Integrity",
        "Affected assets": "Data"
    },
    {
        "Threat Category": "Legal",
        "Threat": "Compromise privacy during data operations",
        "Description": "Data manipulation or erroneous handling during Processes like Data Exploration or Pre-Processing may lead to intentional or unintentional data breaches, which can raise legal concerns over privacy breaches.",
        "Potential Impact": "Confidentiality",
        "Affected assets": "Data"
    },
     {
        "Threat Category": "Legal",
        "Threat": "Corruption of data indexes",
        "Description": "Data indexes threats manifest when their content becomes corrupted. Corruption may be the result of an attack, system crashes, or loss of network connectivity during index replication. Such events can lead to interruptions in index construction tasks, resulting in partially built and defective indexes. Running out of storage capacity during indexing or replication may even lead to the deletion of an entire data index. Denial-of-service attacks may intentionally corrupt data indexes to decrease data access performance. Timing attacks can be used to infer the presence and size of inaccessible data items.",
        "Potential Impact": "Integrity, Availability",
        "Affected assets": "Artefacts"
    },
    {
        "Threat Category": "Legal",
        "Threat": "Disclosure of personal information",
        "Description": "At all stages of the AI lifecycle, disclosure of personal information (either directly or by means of correlation) is a noteworthy threat. The threat is particularly manifested in the absence of verified data accuracy of sources, lack of data randomization, lack of pseudonymity mechanisms , etc.",
        "Potential Impact": "Confidentiality",
        "Affected assets": "Data, Model, Processes, Actor, Artefacts, Environment/tools"
    },

    {
        "Threat Category": "Legal",
        "Threat": "Lack of data governance policies",
        "Description": "When personal data are processed, the existence of data governance policies is part of data controller’s accountability. The GDPR promotes the implementation of data protection by design measures as a way to be effective in the implementation of data protection principles, and in situation of high risks requires the implementation of a data protection impact assessment (DPIA). Data controllers should identify measurable goals and performance indicators that give evidence, also in a quantitative way, of their level of compliance with the principles and implement a DPIA as default option.",
        "Potential Impact": "Integrity, Confidentiality",
        "Affected assets": "Artefacts, Data"
    },
    {
        "Threat Category": "Legal",
        "Threat": "Lack of data protection compliance of 3rd parties",
        "Description": "Third parties are frequently used in providing and processing data, either directly or by means of libraries and models that they provide. This threat refers to the lack of compliance of third parties with respect to applicable data protection regulations.",
        "Potential Impact": "Confidentiality",
        "Affected assets": "Actor"
    },
    {
        "Threat Category": "Legal",
        "Threat": "Profiling of end users",
        "Description": "Labeling may lend itself to a potential threat to anonymity and privacy by acting as a form of profiling.",
        "Potential Impact": "Confidentiality",
        "Affected assets": "Data"
    },
    {
        "Threat Category": "Legal",
        "Threat": "SLA breach",
        "Description": "In the context of 3rd party dependencies, breach of contractual obligations and Service Level Agreements (SLAs) may lead to degradation of performance or even unavailability of the AI system to perform its operation.",
        "Potential Impact": "Availability",
        "Affected assets": "Environment/tools"
    },
    {
        "Threat Category": "Legal",
        "Threat": "Vendor lock-in",
        "Description": "When considering third parties to AI systems, e.g. cloud providers, data storage, AI libraries, etc., the threat of vendor lock-in involves the reliance on a sole third-party provider without a realistic alternative. While this might not necessarily constitute a cybersecurity threat, the lack of backup and overprovisioning might hamper operations.",
        "Potential Impact": "Availability",
        "Affected assets": "Environment/tools"
    },
    {
        "Threat Category": "Legal",
        "Threat": "Weak requirements analysis",
        "Description": "AI requirements may fail when they are built in isolation from the social circumstances that make AI applications necessary. Specifically, functional requirements of AI systems about executing AI tasks with the needed accuracy may fail by not taking into account the impact of the corresponding inherent bias. Non-functional requirements of AI systems may fail by not considering the severity of information leaks and disclosures that can happen in virtualized high-performance execution environments, or when using untrusted software libraries. This is particularly dangerous for AI systems working in domains like healthcare, biotechnology, financial services, and law.",
        "Potential Impact": "Availability",
        "Affected assets": "Artefacts"
    },
    {
        "Threat Category": "Failures or malfunctions",
        "Threat": "Compromising AI application viability",
        "Description": "This type of threat refers to a lack of understanding of what AI/ML are and how to succeed with the business models.",
        "Potential Impact": "Availability",
        "Affected assets": "Artefacts"
    },
    {
        "Threat Category": "Failures or malfunctions",
        "Threat": "Compromising ML pre-processing",
        "Description": "Flaws or defects of the data and metadata schemata greatly influence the quality of the analysis by applications that use the data. In AI applications, a flawed schema will negatively impact on the quality of the ingested information. Flaws often result from inconsistencies in the use of modeling methodologies.",
        "Potential Impact": "Integrity",
        "Affected assets": "Data, Artefacts"
    },
    {
        "Threat Category": "Failures or malfunctions",
        "Threat": "Corruption of data indexes",
        "Description": "Data indexes threats manifest when their content becomes corrupted. Corruption may be the result of an attack, or due to system crashes or loss of network connectivity during index replication. The same events may cause interruptions of index construction tasks, bringing a partially built (and therefore defective) index to production. Also, running out of storage capacity during indexing or replication may cause an entire data index to be deleted.",
        "Potential Impact": "Integrity, Availability",
        "Affected assets": "Artefacts"
    },
    {
        "Threat Category": "Failures or malfunctions",
        "Threat": "Compromise of model frameworks",
        "Description": "Model frameworks fail when they are misconfigured or offer additional attack vectors with respect to traditional software, firmware, and hardware environments. The ML platform’s data volume and processing requirements mean that the workloads are often handled on the cloud, adding another level of complexity and vulnerability.",
        "Potential Impact": "Integrity",
        "Affected assets": "Environment/tools, Artefacts"
    },
    {
        "Threat Category": "Failures or malfunctions",
        "Threat": "Errors or timely restrictions due to non-reliable data infrastructures",
        "Description": "This type of threat is related to data and computational exposure and/or inadequate capacity that may expose data and compromise privacy preservation.",
        "Potential Impact": "Integrity, Availability",
        "Affected assets": "Environment/tools"
    },
     {
        "Threat Category": "Failures or malfunctions",
        "Threat": "Inadequate/absent data quality checks",
        "Description": "Given the importance of data and the need for data to hold markers of their quality (e.g. in terms of sample size, variances, applied data collection methodologies, real vs synthetic data provenance), the lack of or the inadequacy of data quality checks may lead to poor performance of an AI system.",
        "Potential Impact": "Availability, Confidentiality",
        "Affected assets": "Data, Processes"
    },

    {
        "Threat Category": "Failures or malfunctions",
        "Threat": "Label manipulation or weak labelling",
        "Description": "This threat refers to supervised learning systems, which not infer correctly due to wrong or imprecise data labels. If adversaries can only modify the training labels with some or all knowledge of the target model, they need to find the most vulnerable labels. Random perturbation of labels is one possible attack, while additionally there is the case of adversarial label noise (intentional switching of classification labels leading to deterministic noise, an error that the model cannot capture due to its generalization bias).",
        "Potential Impact": "Confidentiality, Integrity",
        "Affected assets": "Processes, Data"
    },
    {
        "Threat Category": "Failures or malfunctions",
        "Threat": "Lack of documentation",
        "Description": "This threat generally manifests over the course of time. In AI systems, model selection should be made in a framework of accountability and trust, and 'black-box' approaches should be avoided. At any stage, the choice of algorithm parameters should be justified and duly documented. Discarded alternatives should be disclosed, and the consequences of model under-fitting or overfitting should be clearly explained. This set of parameters and design choices is important to be able to identify potential errors (intentional or unintentional). Failure to properly maintain documentation of the AI system threatens to indirectly limit its failsafe operation.",
        "Potential Impact": "Integrity, Availability",
        "Affected assets": "Processes"
    },
    {
        "Threat Category": "Failures or malfunctions",
        "Threat": "ML Model Performance Degradation",
        "Description": "The performance of an AI's ML model may degrade due to data governance policy issues, omissions, or corruption caused by system crashes or loss of network connectivity.",
        "Potential Impact": "Availability",
        "Affected assets": "Processes, Model"
    },

    {
        "Threat Category": "Failures or malfunctions",
        "Threat": "Poor resource planning",
        "Description": "The proper functioning of an AI system may be compromised by the lack of adequate computational resources (storage capacity, transmission speed, computational power). This is particularly relevant in real-time applications and in the health sector. In order to deliver the expected beneficial outcome, it is essential that these resources are correctly dimensioned and allocated, and that the final user is aware of such infrastructural constraints. It is part of the informational accountability of the developer to provide the user, and with prominent means, the list of resources to arrange, and their configuration settings, necessary to avoid failures and impacts on the functioning of an AI system.",
        "Potential Impact": "Integrity, Availability",
        "Affected assets": "Artefacts, Environment/tools"
    },

    {
        "Threat Category": "Failures or malfunctions",
        "Threat": "Scarce data",
        "Description": "AI relies on the availability of consistent and accessible data. The scarcity of data may compromise AI viability and limit its results. Data scarcity can result from deliberate or unintentional actions, especially during data ingestion.",
        "Potential Impact": "Availability",
        "Affected assets": "Data, Processes"
    },

    {
        "Threat Category": "Failures or malfunctions",
        "Threat": "Stream interruption",
        "Description": "This threat relates to the interruption of data streams, particularly during processes like data ingestion and training. Lack of data and data interruption can lead to failures in an AI/ML system.",
        "Potential Impact": "Confidentiality, Integrity, Availability",
        "Affected assets": "Processes"
    },
    {
        "Threat Category": "Failures or malfunctions",
        "Threat": "Weak data governance policies",
        "Description": "In AI applications, data governance policies may fail due to defective data metrics, absence of documentation, and lack of adaptability. Failure to specify data quality metrics for ML training and monitor/record AI data usage can also be problematic.",
        "Potential Impact": "Confidentiality, Integrity",
        "Affected assets": "Artefacts, Data"
    },
    {
        "Threat Category": "Failures or malfunctions",
        "Threat": "Weak requirements analysis",
        "Description": "AI requirements may fail when they are built in isolation from the social circumstances that make AI applications necessary. Specifically, functional requirements of AI systems about executing AI tasks with the needed accuracy may fail by not taking into account the impact of the corresponding inherent bias. Non-functional requirements of AI systems may fail by not considering the severity of information leaks and disclosures that can happen in virtualized high performance execution environments, or when using untrusted software libraries. This is particularly dangerous for AI systems working in domains like healthcare, biotechnology, financial services and law.",
        "Potential Impact": "Integrity, Confidentiality, Availability",
        "Affected assets": "Artefacts"
    },
    {
        "Threat Category": "Failures or malfunctions",
        "Threat": "3rd party provider failure",
        "Description": "Failures or malfunctions of 3rd party providers (e.g., cloud providers, data storage providers, AI as Service providers) may lead to the unavailability of an AI system and improper or delayed operation.",
        "Potential Impact": "Availability, Confidentiality",
        "Affected assets": "Environment/tools"
    },

    {
        "Threat Category": "Eavesdropping Interception Hijacking",
        "Threat": "Data inference",
        "Description": "This threat may be exploited by the Data Providers and Model Providers, and can lad to inference of data. Evidently, in the case of personal data, such inference raises concerns in terms of privacy and/or discrimination.",
        "Potential Impact": "Confidentiality",
        "Affected assets": "Data"
    },
    {
        "Threat Category": "Eavesdropping Interception Hijacking",
        "Threat": "Data theft",
        "Description": "This threat may manifest during the transportation of data, during Processes like Data Ingestion and in the context of access to data storage means. In these cases, data may be intercepted and stole.",
        "Potential Impact": "Confidentiality, Integrity",
        "Affected assets": "Data"
    },
    {
        "Threat Category": "Eavesdropping Interception Hijacking",
        "Threat": "Model Disclosure",
        "Description": "Threat of leaking information about trained and/or tuned models internal parameters and other settings of models.",
        "Potential Impact": "Confidentiality",
        "Affected assets": "Model"
    },
    {
        "Threat Category": "Eavesdropping Interception Hijacking",
        "Threat": "Stream interruption",
        "Description": "This threat relates to the interruption of data streams, particularly during processes like data ingestion and training. Lack of data and data interruption can lead to failures in an AI/ML system.",
        "Potential Impact": "Confidentiality, Integrity, Availability",
        "Affected assets": "Processes"
    },

    {
        "Threat Category": "Eavesdropping Interception Hijacking",
        "Threat": "Weak encryption",
        "Description": "In the context of AI, this threat is related to assets of the Processes category, and refers to potential eavesdropping of data or hijacking of communications in the case of data transfers/storage/processing. The threat when materialized may expose data sets and even personal and sensitive information.",
        "Potential Impact": "Confidentiality, Integrity",
        "Affected assets": "Data, Processes, Environment/tools"
    },

     {
        "Threat Category": "Physical attacks",
        "Threat": "Communication networks tampering",
        "Description": "Tampering of communication networks may lead to their unavailability and is a major threat that may be exploited by adversaries. The corresponding outages may lead to delays in decision-making, delays in the processing of data streams, and entire AI systems being placed offline. Moreover, side-channel attacks may expose private and sensitive information that traverses communication networks.",
        "Potential Impact": "Confidentiality, Availability",
        "Affected assets": "Environment/tools"
    },
    {
        "Threat Category": "Physical attacks",
        "Threat": "Errors or timely restrictions due to non-reliable data infrastructures",
        "Description": "This type of threat is related to data and computational exposure and/or inadequate capacity that may expose data and compromise privacy preservation.",
        "Potential Impact": "Integrity, Availability",
        "Affected assets": "Environment/tools"
    },
    {
        "Threat Category": "Physical attacks",
        "Threat": "Infrastructure/system physical attacks",
        "Description": "Physical attacks against infrastructure (IT and corporate services) that support AI systems’ operation and maintenance are a potential threat. Manifestation of this threat leads to degraded performance and even unavailability. The threat manifests by physically attacking, dismantling, or even destroying the actual physical infrastructure.",
        "Potential Impact": "Availability",
        "Affected assets": "Environment/tools"
    },
    {
        "Threat Category": "Physical attacks",
        "Threat": "Model Sabotage",
        "Description": "Sabotaging the model is a nefarious threat that refers to exploitation or physical damage to hardware hosting libraries and machine learning platforms that host or supply AI/ML services and systems.",
        "Potential Impact": "Availability",
        "Affected assets": "Environment/tools"
    },
    {
        "Threat Category": "Physical attacks",
        "Threat": "Sabotage",
        "Description": "Sabotage involves intentionally destroying or maliciously affecting the IT infrastructure that supports AI systems.",
        "Potential Impact": "Availability",
        "Affected assets": "Environment/tools"
    },

     {
        "Threat Category": "Outages",
        "Threat": "Communication networks outages",
        "Description": "Outages to communication networks may adversely influence the performance and operation of AI systems. Such outages may lead to delays in decision-making, delays in the processing of data streams, and entire AI systems being placed offline.",
        "Potential Impact": "Availability",
        "Affected assets": "Environment/tools"
    },
    {
        "Threat Category": "Outages",
        "Threat": "Infrastructure/system outages",
        "Description": "Outages to infrastructure (IT and corporate services) that support AI systems’ operation and maintenance. Manifestation of this threat leads to degraded performance and even unavailability.",
        "Potential Impact": "Availability",
        "Affected assets": "Environment/tools"
    },
    {
        "Threat Category": "Disasters",
        "Threat": "Environmental phenomena (heating, cooling, climate change)",
        "Description": "Environmental phenomena may adversely influence the operation of IT infrastructure and hardware systems that support AI systems. Climate change, in particular, has been consistently highlighted in ENISA reports on telecom incident reporting as the main cause of telecom outages. Such outages may lead to delays in decision-making, delays in the processing of data streams, and entire AI systems being placed offline.",
        "Potential Impact": "Availability",
        "Affected assets": "Environment/tools"
    },
    {
        "Threat Category": "Disasters",
        "Threat": "Natural disasters (earthquake, flood, fire, etc.)",
        "Description": "Natural disasters may lead to unavailability or destruction of the IT infrastructures and hardware that enables the operation, deployment, and maintenance of AI systems.",
        "Potential Impact": "Availability",
        "Affected assets": "Environment/tools"
    }


]