# docker run --rm --gpus=all -d -v ollama_data:/root/.ollama -p 11343:11434 --name ollamaEval ollama/ollama

# docker exec -it ollamaEval ollama run llama3.2

threatsTM1 = [{'Threat': 'Runtime Model Poisoning (Manipulating the model itself or its input/output logic)', 'Description': 'This threat involves manipulating the behavior of the model by altering the parameters within the live system itself. These parameters represent the regularities extracted during the training process for the model to use in its task, such as neural network weights. Alternatively, compromising the model’s input or output logic can also change its behavior or deny its service.'}, {'Threat': 'Broad Model Poisoning Development-time', 'Description': 'Model poisoning in the broad sense is manipulating model behavior by altering training data, code, configuration, or model parameters during development-time. There are roughly two categories of data poisoning: 1) Backdoors - which trigger unwanted responses to specific inputs (e.g. a money transaction is wrongfully marked as NOT fraud because it has a specific amount of money for which the model has been manipulated to ignore). Other name: Trojan attack. 2) Sabotage: data poisoning leads to unwanted results for regular inputs, leading to e.g. business continuity problems or safety issues. Sabotage data poisoning attacks are relatively easy to detect because they occur for regular inputs, but backdoor data poisoning only occurs for really specific inputs and is therefore hard to detect. There is no code to review in a model to look for backdoors, the model parameters make no sense to the human eye, and testing is typically done using normal cases, with blind spots for backdoors. This is the intention of attackers - to bypass regular testing. The best approach is 1) to prevent poisoning by protecting development-time, and 2) to assume training data has been compromised.'}, {'Threat': 'Data Poisoning', 'Description': 'The attacker manipulates (training) data to affect the algorithm’s behavior. Also called causative attacks. There are multiple ways to do this (attack vectors): 1) Changing the data while in storage during development-time (e.g. by hacking the database). 2) Changing the data while in transit to the storage (e.g. by hacking into a data connection). 3) Changing the data while at the supplier, before the data is obtained from the supplier. 4) Changing the data while at the supplier, where a model is trained and then that model is obtained from the supplier. 5) Manipulating data entry in operation, for example by creating fake accounts to enter positive reviews for products, making these products get recommended more often.'}, {'Threat': 'Evasion after data poisoning', 'Description': 'After training data has been poisoned (see data poisoning section), specific input (called backdoors or triggers) can lead to unwanted model output.'}, {'Threat': 'Sensitive data output from model', 'Description': 'The output of the model may contain sensitive data from the training set, for example a large language model (GenAI) generating output including personal data that was part of its training set. Furthermore, GenAI can output other types of sensitive data, such as copyrighted text or images (see Copyright). Once training data is in a GenAI model, original variations in access rights cannot be controlled anymore. (OWASP for LLM 06). The disclosure is caused by an unintentional fault of including this data, and exposed through normal use or through provocation by an attacker using the system. See MITRE ATLAS - LLM Data Leakage.'}, {'Threat': 'Model inversion and Membership inference', 'Description': 'Model inversion (or data reconstruction) occurs when an attacker reconstructs a part of the training set by intensive experimentation during which the input is optimized to maximize indications of confidence level in the output of the model. Membership inference is presenting a model with input data that identifies something or somebody (e.g. a personal identity or a portrait picture), and using any indication of confidence in the output to infer the presence of that something or somebody in the training set.'}, {'Threat': 'Failure or malfunction of AI-specific elements through use', 'Description': 'Specific input to the model leads to availability issues (system being very slow or unresponsive, also called denial of service), typically caused by excessive resource usage. The failure occurs from frequency, volume, or the content of the input. See MITRE ATLAS - Denial of ML service. For example: A sponge attack or energy latency attack provides input that is designed to increase the computation time of the model, potentially causing a denial of service. See article on sponge examples.'}]
#threatsTM2 = [{'Threat': 'Runtime Model Poisoning (Manipulating the model itself or its input/output logic)', 'Description': 'This threat involves manipulating the behavior of the model by altering the parameters within the live system itself. These parameters represent the regularities extracted during the training process for the model to use in its task, such as neural network weights. Alternatively, compromising the model’s input or output logic can also change its behavior or deny its service.'}, {'Threat': 'Broad Model Poisoning Development-time', 'Description': 'Model poisoning in the broad sense is manipulating model behavior by altering training data, code, configuration, or model parameters during development-time. There are roughly two categories of data poisoning: 1) Backdoors - which trigger unwanted responses to specific inputs (e.g. a money transaction is wrongfully marked as NOT fraud because it has a specific amount of money for which the model has been manipulated to ignore). Other name: Trojan attack. 2) Sabotage: data poisoning leads to unwanted results for regular inputs, leading to e.g. business continuity problems or safety issues. Sabotage data poisoning attacks are relatively easy to detect because they occur for regular inputs, but backdoor data poisoning only occurs for really specific inputs and is therefore hard to detect. There is no code to review in a model to look for backdoors, the model parameters make no sense to the human eye, and testing is typically done using normal cases, with blind spots for backdoors. This is the intention of attackers - to bypass regular testing. The best approach is 1) to prevent poisoning by protecting development-time, and 2) to assume training data has been compromised.'}, {'Threat': 'Data Poisoning', 'Description': 'The attacker manipulates (training) data to affect the algorithm’s behavior. Also called causative attacks. There are multiple ways to do this (attack vectors): 1) Changing the data while in storage during development-time (e.g. by hacking the database). 2) Changing the data while in transit to the storage (e.g. by hacking into a data connection). 3) Changing the data while at the supplier, before the data is obtained from the supplier. 4) Changing the data while at the supplier, where a model is trained and then that model is obtained from the supplier. 5) Manipulating data entry in operation, for example by creating fake accounts to enter positive reviews for products, making these products get recommended more often.'}, {'Threat': 'Evasion after data poisoning', 'Description': 'After training data has been poisoned (see data poisoning section), specific input (called backdoors or triggers) can lead to unwanted model output.'}, {'Threat': 'Sensitive data output from model', 'Description': 'The output of the model may contain sensitive data from the training set, for example a large language model (GenAI) generating output including personal data that was part of its training set. Furthermore, GenAI can output other types of sensitive data, such as copyrighted text or images (see Copyright). Once training data is in a GenAI model, original variations in access rights cannot be controlled anymore. (OWASP for LLM 06). The disclosure is caused by an unintentional fault of including this data, and exposed through normal use or through provocation by an attacker using the system. See MITRE ATLAS - LLM Data Leakage.'}, {'Threat': 'Model inversion and Membership inference', 'Description': 'Model inversion (or data reconstruction) occurs when an attacker reconstructs a part of the training set by intensive experimentation during which the input is optimized to maximize indications of confidence level in the output of the model. Membership inference is presenting a model with input data that identifies something or somebody (e.g. a personal identity or a portrait picture), and using any indication of confidence in the output to infer the presence of that something or somebody in the training set.'}, {'Threat': 'Failure or malfunction of AI-specific elements through use', 'Description': 'Specific input to the model leads to availability issues (system being very slow or unresponsive, also called denial of service), typically caused by excessive resource usage. The failure occurs from frequency, volume, or the content of the input. See MITRE ATLAS - Denial of ML service. For example: A sponge attack or energy latency attack provides input that is designed to increase the computation time of the model, potentially causing a denial of service. See article on sponge examples.'}]
threatDesc1 = "The system consists of 2 trust boundaries called: Hospital Web Application, Healthcare Cloud. There is an asset Raw Data called Patient Data which is in the trust boundary Hospital Web Application. The asset Data Collection points to asset Patient Data with an arrow. There is an asset Data Collection called Data Collection which is in the trust boundary Hospital Web Application. The asset Patient points to asset Data Collection with an arrow. There is an asset Service consumers / Model users called Patient. The asset Healthcare API points to asset Pre-processing with an arrow. The asset Healthcare API points to asset Trained Model with an arrow. There is an asset Machine Learning Platforms called Healthcare API which is in the trust boundary Healthcare Cloud. The asset Pre-processing points to asset Data Storage with an arrow. There is an asset Data Exploration/Pre-processing called Pre-processing which is in the trust boundary Healthcare Cloud. The asset Data Storage points to asset Model Training with an arrow. There is an asset Data Storage called Data Storage which is in the trust boundary Healthcare Cloud. The asset Model Training points to asset Trained Model with an arrow. There is an asset Model selection/building, training, and testing called Model Training which is in the trust boundary Healthcare Cloud. There is an asset Trained models called Trained Model which is in the trust boundary Healthcare Cloud. The asset Doctor points to asset Data Collection with an arrow. There is an asset Service consumers / Model users called Doctor. The asset Patient Data points to asset Healthcare API with an arrow"
threatDesc2 = "The system consists of 2 trust boundaries called: AI Banking Provider, E-Banking. The asset Pre-processing points to asset Processed Data with an arrow. There is an asset Data Exploration/Pre-processing called Pre-processing which is in the trust boundary AI Banking Provider. The asset Processed Data points to asset Training Data with an arrow. There is an asset Pre-processed Data Set called Processed Data which is in the trust boundary AI Banking Provider. The asset Transaction Data points to asset Pre-processing with an arrow. There is an asset Raw Data called Transaction Data which is in the trust boundary AI Banking Provider. The asset Training Data points to asset Model Training with an arrow. There is an asset Training Data called Training Data which is in the trust boundary AI Banking Provider. The asset Model Training points to asset Trained Model with an arrow. There is an asset Model selection/building, training, and testing called Model Training which is in the trust boundary AI Banking Provider. There is an asset Trained models called Trained Model which is in the trust boundary AI Banking Provider. The asset Data Scientist points to asset Pre-processing with an arrow. The asset Data Scientist points to asset Model Training with an arrow. There is an asset developer called Data Scientist which is in the trust boundary AI Banking Provider. The asset End User points to asset ML Platform with an arrow. There is an asset End Users called End User which is in the trust boundary E-Banking. The asset ML Platform points to asset Transaction Decision with an arrow. There is an asset Machine Learning Platforms called ML Platform which is in the trust boundary E-Banking. There is an asset Value Proposition and Business Model called Transaction Decision which is in the trust boundary E-Banking. The asset Trained Model points to asset ML Platform with an arrow."

input_text = (
    "### Task Definition: "
    "Analyze the provided system description and update the descriptions of potential threats to explain why each threat is critical to this system. Each updated description should: "
    "1. Explain the threat's importance and relevance to the system based on the provided details (2-4 sentences). "
    "2. Include an example (1-2 sentences) showing how the threat could be exploited, using asset names and trust boundaries. "
    "3. Assign a unique ranking to each threat based on its importance (1 = most critical, larger numbers = less critical). Ensure no duplicate rankings by explicitly comparing and adjusting as needed. "
    
    "### Style and Tone: "
    "Use concise, simple, and professional language. Focus only on the provided system description and threats. "
    "Do not add new threats, modify the provided threats, or introduce unrelated information. "
    
    "### Handling Edge Cases: "
    "If a threat lacks sufficient context in its description, leave the original description unchanged and assign it the lowest ranking. "
    
    "### Input Details: "
    f"System Description: {threatDesc1}"
    f"Threats: {str(threatsTM1)}. "
    
    "### Output Requirements: "
    "Return only the JSON-formatted list of threats. Do not include any additional text, such as comments, explanations, or introductions, before or after the JSON output. The JSON should strictly adhere to this structure:"
    "- 'Threat': The name of the threat. "
    '- "Description": An updated description with the explanation and example. '
    '- "Ranking": The unique ranking of the threat. '
    
    "### Example Output: "
    '[{"Threat": "Threat Name Placeholder",'
    '"Description": "This threat impacts [assetname of system description] by [action placeholder], leading to [consequence placeholder]. For example, [example action placeholder] could exploit [trustboundary name of system description].",'
    '"Ranking": 1}, '
    '{"Threat": "[Another Threat Name Placeholder]",'
    '"Description": "This threat allows adversaries to [action placeholder] in [assetname of system description], causing [consequence placeholder]. For instance, [example action placeholder] might leverage [trustboundary name of system description] to compromise security.",'
    '"Ranking": 2}, ...]'
)

from langchain_ollama import OllamaLLM  # type: ignore
import json

OLLAMA_MODEL="deepseek-r1:1.5b"
OLLAMA_URL="localhost"

threatList = [{'Threat Category': 'Runtime Application Security Threat', 'Threat': 'Runtime Model Poisoning (Manipulating the model itself or its input/output logic)', 'Description': 'This threat involves manipulating the behavior of the model by altering the parameters within the live system itself. These parameters represent the regularities extracted during the training process for the model to use in its task, such as neural network weights. Alternatively, compromising the model’s input or output logic can also change its behavior or deny its service.', 'Potential Impact': 'Integrity', 'Affected assets': ['Model', 'Data'], 'potentialKeyThreat': False}, {'Threat Category': 'Development-time Threats', 'Threat': 'Broad Model Poisoning Development-time', 'Description': 'Model poisoning in the broad sense is manipulating model behavior by altering training data, code, configuration, or model parameters during development-time. There are roughly two categories of data poisoning: 1) Backdoors - which trigger unwanted responses to specific inputs (e.g. a money transaction is wrongfully marked as NOT fraud because it has a specific amount of money for which the model has been manipulated to ignore). Other name: Trojan attack. 2) Sabotage: data poisoning leads to unwanted results for regular inputs, leading to e.g. business continuity problems or safety issues. Sabotage data poisoning attacks are relatively easy to detect because they occur for regular inputs, but backdoor data poisoning only occurs for really specific inputs and is therefore hard to detect. There is no code to review in a model to look for backdoors, the model parameters make no sense to the human eye, and testing is typically done using normal cases, with blind spots for backdoors. This is the intention of attackers - to bypass regular testing. The best approach is 1) to prevent poisoning by protecting development-time, and 2) to assume training data has been compromised.', 'Potential Impact': 'Integrity', 'Affected assets': ['Model', 'Data'], 'potentialKeyThreat': False}, {'Threat Category': 'Development-time Threat', 'Threat': 'Data Poisoning', 'Description': 'The attacker manipulates (training) data to affect the algorithm’s behavior. Also called causative attacks. There are multiple ways to do this (attack vectors): 1) Changing the data while in storage during development-time (e.g. by hacking the database). 2) Changing the data while in transit to the storage (e.g. by hacking into a data connection). 3) Changing the data while at the supplier, before the data is obtained from the supplier. 4) Changing the data while at the supplier, where a model is trained and then that model is obtained from the supplier. 5) Manipulating data entry in operation, for example by creating fake accounts to enter positive reviews for products, making these products get recommended more often.', 'Potential Impact': 'Integrity', 'Affected assets': ['Model', 'Data'], 'potentialKeyThreat': False}, {'Threat Category': 'Threat through Use', 'Threat': 'Evasion after data poisoning', 'Description': 'After training data has been poisoned (see data poisoning section), specific input (called backdoors or triggers) can lead to unwanted model output.', 'Potential Impact': 'Integrity', 'Affected assets': ['Model', 'Data'], 'potentialKeyThreat': False}, {'Threat Category': 'Threat through Use', 'Threat': 'Sensitive data output from model', 'Description': 'The output of the model may contain sensitive data from the training set, for example a large language model (GenAI) generating output including personal data that was part of its training set. Furthermore, GenAI can output other types of sensitive data, such as copyrighted text or images (see Copyright). Once training data is in a GenAI model, original variations in access rights cannot be controlled anymore. (OWASP for LLM 06). The disclosure is caused by an unintentional fault of including this data, and exposed through normal use or through provocation by an attacker using the system. See MITRE ATLAS - LLM Data Leakage.', 'Potential Impact': 'Confidentiality', 'Affected assets': ['Model', 'Data'], 'potentialKeyThreat': True}, {'Threat Category': 'Threat through Use', 'Threat': 'Model inversion and Membership inference', 'Description': 'Model inversion (or data reconstruction) occurs when an attacker reconstructs a part of the training set by intensive experimentation during which the input is optimized to maximize indications of confidence level in the output of the model. Membership inference is presenting a model with input data that identifies something or somebody (e.g. a personal identity or a portrait picture), and using any indication of confidence in the output to infer the presence of that something or somebody in the training set.', 'Potential Impact': 'Confidentiality', 'Affected assets': ['Model', 'Data'], 'potentialKeyThreat': True}, {'Threat Category': 'Threat through Use', 'Threat': 'Failure or malfunction of AI-specific elements through use', 'Description': 'Specific input to the model leads to availability issues (system being very slow or unresponsive, also called denial of service), typically caused by excessive resource usage. The failure occurs from frequency, volume, or the content of the input. See MITRE ATLAS - Denial of ML service. For example: A sponge attack or energy latency attack provides input that is designed to increase the computation time of the model, potentially causing a denial of service. See article on sponge examples.', 'Potential Impact': 'Availability', 'Affected assets': ['Model', 'Processes', 'Environments and Tools'], 'potentialKeyThreat': False}]


llm = OllamaLLM(model=OLLAMA_MODEL, base_url=OLLAMA_URL)

output =  llm.invoke(input_text)

print("Raw LLM Output:")
print(output)

try:
    output = json.loads(output)
    for o in output:
        for t in threatList:
            if o["Threat"] == t["Threat"]:
                t["Description"] = o["Description"]
                if o["Ranking"]==1:
                    t["Description"] = t["Description"] + " This is a Rank 1 threat, which means it is the most important to address. "
                    print(f"Rank 1 was given to Threat: {o['Threat']}.")
                else:
                    t["Description"] = t["Description"] + f" This is a Rank {o['Ranking']} threat, less critical than Rank 1 but still highly important. "
                    print(f"Rank {o['Ranking']} was given to Threat: {o['Threat']}.")
                break   
    
    print("---------------------------------------------------------------------------")
    print("True - Went through")
except json.JSONDecodeError as e:
    print("False - Failed")