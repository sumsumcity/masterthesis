### 2.1.1. Closed-box evasion
>Category: threat through use  

Black box or closed-box attacks are methods where an attacker crafts an input to exploit a model without having any internal knowledge or access to that model's implementation, including code, training set, parameters, and architecture. The term "black box" reflects the attacker's perspective, viewing the model as a 'closed box' whose internal workings are unknown. This approach often requires experimenting with how the model responds to various inputs, as the attacker navigates this lack of transparency to identify and leverage potential vulnerabilities.
Since the attacker does not have access to the inner workings of the model, he cannot calculate the internal model gradients to efficiently create the adversarial inputs - in contrast to white-box or open-box attacks (see 2.1.2. Open-box evasion).

Black box attack strategies are:
- Transferability-Based Attacks:
  Attackers can execute a transferability-based black box attack by first creating adversarial examples using a surrogate model, a copy or approximation of the closed-box target model, and then applying these adversarial examples to the target model. This approach leverages the concept of an open-box evasion attack, where the attacker utilizes the internals of a surrogate model to construct a successful attack. The goal is to create adversarial examples that will 'hopefully' transfer to the original target model, even though the surrogate may be internally different from the target. The likelihood of a successful transfer is generally higher when the surrogate model closely resembles the target model in terms of complexity and structure. However, it's noted that even attacks developed using simpler surrogate models tend to transfer effectively. To maximize similarity and therefore the effectiveness of the attack, one approach is to reverse-engineer a version of the target model, creating a surrogate that mirrors the target as closely as possible. This strategy is grounded in the rationale that many adversarial examples are inherently transferable across different models, particularly when they share similar architectures or training data. This method of attack, including the creation of a surrogate model through model theft.

- Query-Based Attacks:
  In query-based black box attacks, an attacker systematically queries the target model using carefully designed inputs and observes the resulting outputs to search for variations of input that lead to a false decision of the model.
This approach enables the attacker to indirectly reconstruct or estimate the model's decision boundaries, thereby facilitating the creation of inputs that can mislead the model.
These attacks are categorized based on the type of output the model provides:
  - Desicion-based (or Label-based) attacks: where the model only reveals the top prediction label
  - Score-based attacks: where the model discloses a score (like a softmax score), often in the form of a vector indicating the top-k predictions.In research typically models which output the whole vector are evaluated, but the output could also be restricted to e.g. top-10 vector. The confidence scores provide more detailed feedback about how close the adversarial example is to succeeding, allowing for more precise adjustments. In a score-based scenario an attacker can for example approximate the gradient by evaluating the objective function values at two very close points.