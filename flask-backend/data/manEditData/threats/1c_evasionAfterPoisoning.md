### 2.1.3. Evasion after data poisoning
>Category: threat through use  

After training data has been poisoned, specific input  (called _backdoors_ or _triggers_) can lead to unwanted model output.